apiVersion: v1
kind: ConfigMap
metadata:
  name: storage-exporter-script
  namespace: monitoring
data:
  exporter.py: |
    #!/usr/bin/env python3
    import time
    import json
    import subprocess
    import sys
    from http.server import HTTPServer, BaseHTTPRequestHandler
    import threading
    import os
    import re

    class StorageMetrics:
        def __init__(self):
            self.metrics = {}
            self.last_update = 0
            
        def get_storage_metrics(self):
            try:
                # Get volume info from /proc/mounts and /sys/block
                volumes = {}
                
                # Parse mounted volumes
                with open('/proc/mounts', 'r') as f:
                    for line in f:
                        parts = line.strip().split()
                        if len(parts) >= 3:
                            device, mount_point, fs_type = parts[0], parts[1], parts[2]
                            if mount_point.startswith('/var/lib/kubelet/pods'):
                                # Extract volume info
                                volume_match = re.search(r'/volumes/([^/]+)/([^/]+)', mount_point)
                                if volume_match:
                                    volume_type, volume_name = volume_match.groups()
                                    volumes[volume_name] = {
                                        'device': device,
                                        'mount_point': mount_point,
                                        'fs_type': fs_type,
                                        'type': 'ssd'  # Default assumption
                                    }

                # Get I/O stats from /proc/diskstats
                iostats = {}
                with open('/proc/diskstats', 'r') as f:
                    for line in f:
                        parts = line.strip().split()
                        if len(parts) >= 14:
                            device = parts[2]
                            read_ios = int(parts[3])
                            read_sectors = int(parts[5])
                            write_ios = int(parts[7])
                            write_sectors = int(parts[9])
                            io_time = int(parts[12])
                            
                            iostats[device] = {
                                'read_iops': read_ios,
                                'write_iops': write_ios,
                                'read_bytes': read_sectors * 512,
                                'write_bytes': write_sectors * 512,
                                'io_time_ms': io_time
                            }

                # Generate mock realistic data for demo
                mock_volumes = [
                    {
                        'name': 'pvc-database-primary',
                        'namespace': 'production',
                        'type': 'ssd',
                        'capacity_gb': 500,
                        'used_gb': 342,
                        'read_iops': 2500 + (time.time() % 100),
                        'write_iops': 1800 + (time.time() % 80),
                        'read_throughput_mbps': 125 + (time.time() % 25),
                        'write_throughput_mbps': 95 + (time.time() % 20),
                        'read_latency_ms': 0.5 + (time.time() % 0.3),
                        'write_latency_ms': 0.8 + (time.time() % 0.4),
                        'status': 'healthy'
                    },
                    {
                        'name': 'pvc-cache-redis',
                        'namespace': 'production',
                        'type': 'nvme',
                        'capacity_gb': 100,
                        'used_gb': 45,
                        'read_iops': 8500 + (time.time() % 200),
                        'write_iops': 6200 + (time.time() % 150),
                        'read_throughput_mbps': 450 + (time.time() % 50),
                        'write_throughput_mbps': 380 + (time.time() % 40),
                        'read_latency_ms': 0.2 + (time.time() % 0.1),
                        'write_latency_ms': 0.3 + (time.time() % 0.2),
                        'status': 'healthy'
                    },
                    {
                        'name': 'pvc-logs-elasticsearch',
                        'namespace': 'monitoring',
                        'type': 'hdd',
                        'capacity_gb': 2000,
                        'used_gb': 1650,
                        'read_iops': 150 + (time.time() % 30),
                        'write_iops': 100 + (time.time() % 20),
                        'read_throughput_mbps': 80 + (time.time() % 15),
                        'write_throughput_mbps': 60 + (time.time() % 10),
                        'read_latency_ms': 5 + (time.time() % 2),
                        'write_latency_ms': 8 + (time.time() % 3),
                        'status': 'degraded'
                    }
                ]
                
                return mock_volumes
                
            except Exception as e:
                print(f"Error getting storage metrics: {e}")
                return []

        def to_prometheus(self):
            volumes = self.get_storage_metrics()
            metrics = []
            
            for vol in volumes:
                labels = f'volume="{vol["name"]}",namespace="{vol["namespace"]}",type="{vol["type"]}",status="{vol["status"]}"'
                
                metrics.extend([
                    f'storage_volume_capacity_bytes{{{labels}}} {vol["capacity_gb"] * 1024 * 1024 * 1024}',
                    f'storage_volume_used_bytes{{{labels}}} {vol["used_gb"] * 1024 * 1024 * 1024}',
                    f'storage_volume_available_bytes{{{labels}}} {(vol["capacity_gb"] - vol["used_gb"]) * 1024 * 1024 * 1024}',
                    f'storage_volume_usage_percent{{{labels}}} {(vol["used_gb"] / vol["capacity_gb"]) * 100}',
                    f'storage_volume_read_iops{{{labels}}} {vol["read_iops"]}',
                    f'storage_volume_write_iops{{{labels}}} {vol["write_iops"]}',
                    f'storage_volume_read_throughput_bytes{{{labels}}} {vol["read_throughput_mbps"] * 1024 * 1024}',
                    f'storage_volume_write_throughput_bytes{{{labels}}} {vol["write_throughput_mbps"] * 1024 * 1024}',
                    f'storage_volume_read_latency_seconds{{{labels}}} {vol["read_latency_ms"] / 1000}',
                    f'storage_volume_write_latency_seconds{{{labels}}} {vol["write_latency_ms"] / 1000}',
                ])
            
            return '\n'.join(metrics)

    storage_metrics = StorageMetrics()

    class MetricsHandler(BaseHTTPRequestHandler):
        def do_GET(self):
            if self.path == '/metrics':
                metrics_output = storage_metrics.to_prometheus()
                self.send_response(200)
                self.send_header('Content-Type', 'text/plain; charset=utf-8')
                self.end_headers()
                self.wfile.write(metrics_output.encode('utf-8'))
            elif self.path == '/health':
                self.send_response(200)
                self.send_header('Content-Type', 'text/plain')
                self.end_headers()
                self.wfile.write(b'OK')
            else:
                self.send_response(404)
                self.end_headers()

        def log_message(self, format, *args):
            pass  # Suppress access logs

    if __name__ == '__main__':
        server = HTTPServer(('0.0.0.0', 9101), MetricsHandler)
        print("Storage metrics exporter starting on :9101")
        server.serve_forever()
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: storage-exporter
  namespace: monitoring
  labels:
    app: storage-exporter
spec:
  selector:
    matchLabels:
      app: storage-exporter
  template:
    metadata:
      labels:
        app: storage-exporter
      annotations:
        prometheus.io/scrape: 'true'
        prometheus.io/port: '9101'
        prometheus.io/path: '/metrics'
    spec:
      hostPID: true
      hostNetwork: false
      containers:
      - name: storage-exporter
        image: python:3.11-alpine
        command: ['python3', '/scripts/exporter.py']
        ports:
        - containerPort: 9101
          name: metrics
        resources:
          requests:
            memory: 50Mi
            cpu: 50m
          limits:
            memory: 100Mi
            cpu: 100m
        volumeMounts:
        - name: script
          mountPath: /scripts
          readOnly: true
        - name: proc
          mountPath: /host/proc
          readOnly: true
        - name: sys
          mountPath: /host/sys
          readOnly: true
        - name: kubelet-pods
          mountPath: /var/lib/kubelet/pods
          readOnly: true
        env:
        - name: PROC_PATH
          value: /host/proc
        - name: SYS_PATH
          value: /host/sys
        livenessProbe:
          httpGet:
            path: /health
            port: 9101
          initialDelaySeconds: 10
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /health
            port: 9101
          initialDelaySeconds: 5
          periodSeconds: 10
      volumes:
      - name: script
        configMap:
          name: storage-exporter-script
          defaultMode: 0755
      - name: proc
        hostPath:
          path: /proc
      - name: sys
        hostPath:
          path: /sys
      - name: kubelet-pods
        hostPath:
          path: /var/lib/kubelet/pods
      tolerations:
      - operator: Exists
---
apiVersion: v1
kind: Service
metadata:
  name: storage-exporter
  namespace: monitoring
  labels:
    app: storage-exporter
  annotations:
    prometheus.io/scrape: 'true'
    prometheus.io/port: '9101'
spec:
  type: ClusterIP
  clusterIP: None
  selector:
    app: storage-exporter
  ports:
  - name: metrics
    port: 9101
    targetPort: 9101